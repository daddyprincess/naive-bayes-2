{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886d27a3-385c-47bb-9362-3bb302f7d3ec",
   "metadata": {},
   "source": [
    "## Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb40654-80af-4850-be22-fe069e58a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use \n",
    "conditional probability. In this case, we are looking for the conditional probability of being a smoker (S) given that the\n",
    "employee uses the health insurance plan (H), denoted as P(S|H).\n",
    "\n",
    "We are given two pieces of information:\n",
    "\n",
    "The probability that an employee uses the health insurance plan, P(H) = 0.70 (70%).\n",
    "The probability that an employee is a smoker given that they use the health insurance plan, P(S|H) = 0.40 (40%).\n",
    "We can use the conditional probability formula:\n",
    "\n",
    "        P(S∣H) = P(S∩H) / P(H)\n",
    "\n",
    "Here:\n",
    "\n",
    "    ~P(S|H) is the probability of being a smoker given using the health insurance plan.\n",
    "    ~P(S ∩ H) is the joint probability of being both a smoker and using the health insurance plan.\n",
    "    ~P(H) is the probability of using the health insurance plan.\n",
    "    \n",
    "We already know P(S|H) and P(H). So, we can calculate P(S ∩ H) using the formula:\n",
    "     P(S∩H)=P(S∣H)⋅P(H)=0.40⋅0.70=0.28\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.28 or 28%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62015138-a00b-440f-8845-6996695dbf85",
   "metadata": {},
   "source": [
    "## Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f376f-256a-48fc-be47-1db596e307f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes classifier, and they are \n",
    "primarily used for different types of data and applications. Here are the key differences between them:\n",
    "\n",
    "1.Type of Data:\n",
    "\n",
    "    ~Bernoulli Naive Bayes: It is typically used for binary or Boolean data, where each feature represents the presence (1) \n",
    "     or absence (0) of a particular attribute or term. It's commonly used in text classification for tasks like spam\n",
    "    detection, where you're interested in whether certain words are present in a document.\n",
    "    ~Multinomial Naive Bayes: It is used for discrete data, particularly when dealing with counts or frequencies. In text \n",
    "     classification, it's used when you want to work with term frequencies (e.g., word counts) within documents.\n",
    "        \n",
    "2.Representation of Features:\n",
    "\n",
    "    ~Bernoulli Naive Bayes: Features are binary, representing whether a particular attribute or term is present or absent.\n",
    "     It focuses on the presence or absence of features.\n",
    "    ~Multinomial Naive Bayes: Features are represented by integer counts or frequencies, typically non-negative integers.\n",
    "    It deals with the frequency of features within a document or data point.\n",
    "     \n",
    "3.Calculation of Probabilities:\n",
    "\n",
    "    ~Bernoulli Naive Bayes: Calculates probabilities based on the presence or absence of features using a Bernoulli\n",
    "     distribution. It models whether or not a feature occurs in a document.\n",
    "    ~Multinomial Naive Bayes: Calculates probabilities based on the frequency of features using a Multinomial distribution.\n",
    "     It models the count of occurrences of each feature in a document.\n",
    "        \n",
    "4.Use Cases:\n",
    "\n",
    "    ~Bernoulli Naive Bayes: Useful for text classification tasks where you want to know if specific terms (features) are\n",
    "     present or not. For example, spam or not spam classification based on the presence of certain keywords.\n",
    "    ~Multinomial Naive Bayes: Commonly used in text classification for tasks where you want to consider the frequency or\n",
    "    count of words within documents, such as sentiment analysis or document categorization.\n",
    "    \n",
    "5.Smoothing:\n",
    "\n",
    "    ~Bernoulli Naive Bayes: Often uses smoothing techniques like Laplace smoothing (add-one smoothing) to handle cases\n",
    "     where certain features are absent in some documents.\n",
    "    ~Multinomial Naive Bayes: Also commonly employs smoothing methods to handle zero-count issues when estimating\n",
    "     probabilities.\n",
    "        \n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your data and the\n",
    "specific requirements of your classification problem. If your data consists of binary features or you're interested in the\n",
    "presence/absence of features, Bernoulli Naive Bayes may be more suitable. On the other hand, if your data involves counts\n",
    "or frequencies of features, Multinomial Naive Bayes is often the better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84c208-81a8-4911-9006-84bfcc176a10",
   "metadata": {},
   "source": [
    "## Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19eb23-7482-4f58-9c76-de346072a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes, like other variants of the Naive Bayes classifier, typically assumes that the absence of a feature\n",
    "(i.e., a missing value) is an informative part of the data. In the context of Bernoulli Naive Bayes, this means that it\n",
    "assumes the absence of a binary feature (i.e., a missing value) is meaningful and can contribute to the classification\n",
    "process.\n",
    "\n",
    "Here's how Bernoulli Naive Bayes handles missing values:\n",
    "\n",
    "1.Absence of a Feature is Treated as a Feature:\n",
    "\n",
    "    ~When a feature is missing (i.e., not observed or not present in the data), Bernoulli Naive Bayes treats it as if the \n",
    "     feature has a value of 0, indicating its absence.\n",
    "        \n",
    "2.Feature Absence and Presence Influence Probabilities:\n",
    "\n",
    "    ~In Bernoulli Naive Bayes, the absence or presence of features (including missing values) is used to calculate\n",
    "     probabilities. It considers both cases when estimating the likelihood of each feature for each class.\n",
    "        \n",
    "3.Smoothing for Missing Features:\n",
    "\n",
    "    ~Bernoulli Naive Bayes often employs smoothing techniques, such as Laplace smoothing (add-one smoothing), to handle \n",
    "     cases where certain features are absent in some documents. Smoothing helps prevent zero probabilities in the \n",
    "    calculations, which can cause problems during classification.\n",
    "    \n",
    "In practice, when using Bernoulli Naive Bayes with missing values, you should handle missing data appropriately before\n",
    "applying the classifier. Common techniques for handling missing data include:\n",
    "\n",
    "1.Imputation: You can impute missing values by replacing them with a specific value, such as 0, to indicate the absence of \n",
    "  a feature. This aligns with the assumption made by Bernoulli Naive Bayes.\n",
    "\n",
    "2.Data Preprocessing: Ensure that your dataset is properly preprocessed to account for missing values. Depending on the \n",
    "  nature of your data, you may need to consider imputation strategies or treat missing values as a separate category if\n",
    "appropriate.\n",
    "\n",
    "3.Smoothing: As mentioned earlier, smoothing techniques can help address issues related to missing features. By applying\n",
    "  Laplace smoothing or similar methods, you can mitigate the impact of missing values on probability calculations.\n",
    "\n",
    "It's important to note that the treatment of missing values can have a significant impact on the performance of a Bernoulli \n",
    "Naive Bayes classifier. The choice of how to handle missing data should be based on the characteristics of your dataset and\n",
    "the specific requirements of your classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c42e4-d3c6-4980-881c-ce49f14d1078",
   "metadata": {},
   "source": [
    "## Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99bd23-115c-4321-99c5-fb7568b4aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes\n",
    "classifier that is well-suited for continuous data where the features are assumed to follow a Gaussian (normal) distribution\n",
    "within each class. While it's often used for binary classification problems, it can also be extended to handle multi-class\n",
    "classification tasks.\n",
    "\n",
    "In multi-class classification using Gaussian Naive Bayes, you apply the following general steps:\n",
    "\n",
    "1.Data Preparation:\n",
    "\n",
    "    ~Collect and preprocess your dataset.\n",
    "    ~Ensure that your features are continuous variables.\n",
    "    \n",
    "2.Model Training:\n",
    "\n",
    "    ~Calculate the mean and variance of each feature for each class in your training data.\n",
    "    ~Estimate the class priors (the probabilities of each class occurring) based on the relative frequencies of the classes\n",
    "     in the training data.\n",
    "        \n",
    "3.Classification:\n",
    "\n",
    "    ~When making predictions for a new data point, calculate the posterior probability of each class using the Gaussian \n",
    "     probability density function for each feature and the estimated class priors.\n",
    "    ~Assign the data point to the class with the highest posterior probability.\n",
    "    \n",
    "The formula for calculating the Gaussian probability density function for a feature in a given class is:\n",
    "\n",
    "            P(xi∣y) = 1/2πσ2 y,i exp(− (xi−μy,i)2 / 2σ 2 y,i)\n",
    "\n",
    "Where:\n",
    "\n",
    "    ~P(x∣y) is the probability of feature xi given class y.\n",
    "    ~μ y,i is the mean of feature xi for class y.\n",
    "    ~σ2 y,i is the variance of feature xi for class y.\n",
    "    \n",
    "After calculating the probability of each feature for each class, you multiply these probabilities together (assuming\n",
    "feature independence as per the Naive Bayes assumption) to obtain the class's posterior probability.\n",
    "\n",
    "Finally, you assign the new data point to the class with the highest posterior probability.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can be extended to handle multi-class classification by calculating the probabilities for\n",
    "each class and selecting the class with the highest probability as the predicted class for a given data point. It's a \n",
    "straightforward and efficient algorithm for multi-class classification problems with continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982784a5-4f3f-478b-882a-1ac7b2dd4671",
   "metadata": {},
   "source": [
    "## Q5. Assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620df92-cfa5-42e2-9639-25972f48c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "I can guide you through the process of performing this assignment. It involves downloading the \"Spambase Data Set\" from the\n",
    "UCI Machine Learning Repository, implementing Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes\n",
    "classifiers using scikit-learn, evaluating their performance using 10-fold cross-validation, reporting performance metrics,\n",
    "discussing the results, and providing a conclusion.\n",
    "\n",
    "Here are the steps you can follow:\n",
    "\n",
    "Step 1: Data Download and Preparation\n",
    "\n",
    "1.Download the \"Spambase Data Set\" from the UCI Machine Learning Repository using the provided link: Spambase Data Set.\n",
    "\n",
    "2.Preprocess the dataset as needed. This may include handling missing values, splitting it into features and labels, and any\n",
    "  other necessary data preparation steps.\n",
    "\n",
    "Step 2: Implementation of Naive Bayes Classifiers\n",
    "\n",
    "1.Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn. You can\n",
    " use the BernoulliNB, MultinomialNB, and GaussianNB classes provided by scikit-learn.\n",
    "    \n",
    "Step 3: Cross-Validation and Performance Metrics\n",
    "\n",
    "1.Perform 10-fold cross-validation for each classifier using the dataset. You can use the cross_val_score function from\n",
    "  scikit-learn to achieve this.\n",
    "\n",
    "2.Calculate the following performance metrics for each classifier:\n",
    "\n",
    "    ~Accuracy\n",
    "    ~Precision\n",
    "    ~Recall\n",
    "    ~F1 Score\n",
    "    \n",
    "Step 4: Results\n",
    "\n",
    "1.Report the performance metrics for each classifier. You can use Python libraries such as NumPy and scikit-learn's \n",
    " classification_report function to calculate and format these metrics.\n",
    "    \n",
    "Step 5: Discussion\n",
    "\n",
    "1.Discuss the results you obtained. Compare the performance of Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian\n",
    " Naive Bayes. Consider factors like the nature of the data and the assumptions of each variant of Naive Bayes. Explain \n",
    "which variant performed the best and why you think that is the case.\n",
    "\n",
    "2.Mention any limitations or observations you made during the evaluation. For example, Naive Bayes assumes independence\n",
    "  between features, which may not hold true in some cases.\n",
    "\n",
    "Step 6: Conclusion\n",
    "\n",
    "1.Summarize your findings, including which Naive Bayes variant performed the best overall and why. Provide some\n",
    "  suggestions for future work, such as exploring feature engineering techniques or trying other machine learning\n",
    "algorithms to improve classification performance.\n",
    "\n",
    "2.Present your assignment report with clear and organized sections for each of the above steps.\n",
    "\n",
    "Feel free to ask for more specific guidance or assistance with any of these steps as you work on the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
